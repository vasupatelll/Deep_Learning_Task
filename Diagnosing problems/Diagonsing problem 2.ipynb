{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053ab5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ccede7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7a5e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "389cea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b70c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0221152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model first, from input to output\n",
    "\n",
    "# this is a super deep model, cool!\n",
    "n_units = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf6074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a68469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8c59001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30cf0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "314330b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5ba5b",
   "metadata": {},
   "source": [
    "Problem dignoised using relu rather than using sigmoid. As sigmoid returns value between the range of 0 to 1, due to this gradient was trapped into local minima. Before diagnosing sigmoid activation function was applies which retun its range (0, 1).\n",
    "Whereas, relu returns MAX(0, 1), output of relu does not have a maximum value because it returns 0 if value is negeative. So to minimise loss it helps gradient descent to reach its global minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "645cf2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302622079849243 Accuracy: 0.078125\n",
      "Loss: 2.3007431030273438 Accuracy: 0.109375\n",
      "Loss: 2.304184913635254 Accuracy: 0.09375\n",
      "Loss: 2.2920308113098145 Accuracy: 0.1640625\n",
      "Loss: 2.3046836853027344 Accuracy: 0.1015625\n",
      "Loss: 2.30397891998291 Accuracy: 0.1015625\n",
      "Loss: 2.3020431995391846 Accuracy: 0.046875\n",
      "Loss: 2.290890693664551 Accuracy: 0.140625\n",
      "Loss: 2.3018527030944824 Accuracy: 0.1171875\n",
      "Loss: 2.263622283935547 Accuracy: 0.140625\n",
      "Loss: 1.8177757263183594 Accuracy: 0.2421875\n",
      "Loss: 1.4711976051330566 Accuracy: 0.390625\n",
      "Loss: 1.0134949684143066 Accuracy: 0.5859375\n",
      "Loss: 0.8483679890632629 Accuracy: 0.703125\n",
      "Loss: 0.6489059925079346 Accuracy: 0.8203125\n",
      "Loss: 0.6033318638801575 Accuracy: 0.8515625\n",
      "Loss: 0.4619532525539398 Accuracy: 0.875\n",
      "Loss: 0.790960967540741 Accuracy: 0.7734375\n",
      "Loss: 0.3580614924430847 Accuracy: 0.9140625\n",
      "Loss: 0.26347577571868896 Accuracy: 0.9296875\n",
      "Loss: 0.3106400966644287 Accuracy: 0.9140625\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a9953b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.9272000193595886\n"
     ]
    }
   ],
   "source": [
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
