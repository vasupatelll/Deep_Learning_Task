{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70394be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f165ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)\n",
    "\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c29ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model first, from input to output\n",
    "\n",
    "# 2 layers again\n",
    "n_units = 100\n",
    "n_layers = 2\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f8c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5334704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.310793876647949 Accuracy: 0.0546875\n",
      "Loss: 0.600801944732666 Accuracy: 0.8203125\n",
      "Loss: 0.4528772532939911 Accuracy: 0.859375\n",
      "Loss: 0.34800082445144653 Accuracy: 0.890625\n",
      "Loss: 0.4200986623764038 Accuracy: 0.8671875\n",
      "Loss: 0.28800642490386963 Accuracy: 0.9296875\n",
      "Loss: 0.1946699023246765 Accuracy: 0.9296875\n",
      "Loss: 0.32868507504463196 Accuracy: 0.890625\n",
      "Loss: 0.3479222059249878 Accuracy: 0.9296875\n",
      "Loss: 0.22616975009441376 Accuracy: 0.9609375\n",
      "Loss: 0.24219101667404175 Accuracy: 0.9375\n",
      "Loss: 0.22506286203861237 Accuracy: 0.9453125\n",
      "Loss: 0.1369064599275589 Accuracy: 0.96875\n",
      "Loss: 0.2659686207771301 Accuracy: 0.9296875\n",
      "Loss: 0.14065642654895782 Accuracy: 0.9609375\n",
      "Loss: 0.12561121582984924 Accuracy: 0.9609375\n",
      "Loss: 0.1326696276664734 Accuracy: 0.953125\n",
      "Loss: 0.2205049991607666 Accuracy: 0.9296875\n",
      "Loss: 0.10521896183490753 Accuracy: 0.9609375\n",
      "Loss: 0.16275766491889954 Accuracy: 0.9609375\n",
      "Loss: 0.12569619715213776 Accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    ###################################################\n",
    "    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n",
    "    ###################################################\n",
    "    return logits\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84001e2d",
   "metadata": {},
   "source": [
    "Problem diagnosed with removing softmax above at variable named as \"logits\". As you don't need to do softmax if you don't need probabilities. And using raw logits leads to more numerically stable code. During evaluation, if you are only interested in the highest-probability class, then you can do argmax(vec) on the logits. If you want probability distribution over classes, then you'll need to exponentiate and normalize to 1 - that's what softmax does. After diagnosing, we have more optimum outcome in reducing loss and improved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff4b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.9610000252723694\n"
     ]
    }
   ],
   "source": [
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
