{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2621db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7706367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6e1757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    return images.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    return labels.reshape(-1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08f4e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "train_labels = preprocess_labels(train_labels)\n",
    "test_labels = preprocess_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27000e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n",
    "#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba807c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model first, from input to output\n",
    "\n",
    "# uhm, maybe don't use that many layers actually. 2 is fine!\n",
    "n_units = 100\n",
    "n_layers = 2\n",
    "w_range = 0.1\n",
    "\n",
    "# just set up a \"chain\" of hidden layers\n",
    "# model is represented by a list where each element is a layer,\n",
    "# and each layer is in turn a list of the layer variables (w, b)\n",
    "\n",
    "# first layer goes from n_input to n_hidden\n",
    "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n",
    "                      name=\"w0\")\n",
    "b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n",
    "layers = [[w_input, b_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0e82233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all other hidden layers go from n_hidden to n_hidden\n",
    "for layer in range(n_layers - 1):\n",
    "    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n",
    "                    name=\"w\" + str(layer+1))\n",
    "    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n",
    "    layers.append([w, b])\n",
    "\n",
    "# finally add the output layer\n",
    "w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n",
    "                    name=\"wout\")\n",
    "b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n",
    "layers.append([w_out, b_out])\n",
    "\n",
    "# flatten the layers to get a list of variables\n",
    "all_variables = [variable for layer in layers for variable in layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87332e01",
   "metadata": {},
   "source": [
    "Problem diagnosed with typing error as before it was written as 0. everywhere instead of w_range. Due to this, gradient was  jumping on vast scale, so to control this learning parameters are added. However, learning parameter was 0. which had no effect on jumps on linear algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76d582a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(inputs):\n",
    "    x = inputs\n",
    "    for w, b in layers[:-1]:\n",
    "        x = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "565da38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.295802354812622 Accuracy: 0.15625\n",
      "Loss: 0.6921929121017456 Accuracy: 0.8203125\n",
      "Loss: 0.3275585472583771 Accuracy: 0.90625\n",
      "Loss: 0.34063538908958435 Accuracy: 0.875\n",
      "Loss: 0.49191418290138245 Accuracy: 0.8359375\n",
      "Loss: 0.4102180004119873 Accuracy: 0.8671875\n",
      "Loss: 0.3011515736579895 Accuracy: 0.9296875\n",
      "Loss: 0.18771891295909882 Accuracy: 0.9765625\n",
      "Loss: 0.2108837515115738 Accuracy: 0.953125\n",
      "Loss: 0.158769428730011 Accuracy: 0.9296875\n",
      "Loss: 0.21963739395141602 Accuracy: 0.9453125\n",
      "Loss: 0.21435461938381195 Accuracy: 0.9453125\n",
      "Loss: 0.18348324298858643 Accuracy: 0.96875\n",
      "Loss: 0.1974756121635437 Accuracy: 0.953125\n",
      "Loss: 0.1372903287410736 Accuracy: 0.9765625\n",
      "Loss: 0.15525150299072266 Accuracy: 0.9609375\n",
      "Loss: 0.22184453904628754 Accuracy: 0.9453125\n",
      "Loss: 0.18594668805599213 Accuracy: 0.9453125\n",
      "Loss: 0.12525475025177002 Accuracy: 0.9609375\n",
      "Loss: 0.26480990648269653 Accuracy: 0.921875\n",
      "Loss: 0.19823195040225983 Accuracy: 0.9609375\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "train_steps = 2000\n",
    "for step, (img_batch, lbl_batch) in enumerate(train_data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # here we just run all the layers in sequence via a for-loop\n",
    "        logits = model_forward(img_batch)\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=lbl_batch))\n",
    "\n",
    "    grads = tape.gradient(xent, all_variables)\n",
    "    for grad, var in zip(grads, all_variables):\n",
    "        var.assign_sub(lr*grad)\n",
    "\n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf52a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.11349999904632568\n"
     ]
    }
   ],
   "source": [
    "test_preds = model_forward(test_images)\n",
    "test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n",
    "print(\"Final test accuracy: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
