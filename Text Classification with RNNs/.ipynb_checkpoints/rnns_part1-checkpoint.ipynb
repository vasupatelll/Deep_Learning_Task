{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r-Chp8YyyvxN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZVE7aMvC4iXK"
   },
   "outputs": [],
   "source": [
    "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
    "num_words = 20000\n",
    "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c07u7Z7s4opk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some sequences. words have been replaced with arbitrary index mappings\n",
    "# 1 is a special \"beginning of sequence\" marker\n",
    "# infrequent words have been replaced by the index 2\n",
    "# actual words start with index 4, 3 is never used (???)\n",
    "train_sequences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AD6Elit34sTL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels are simply binary: sentiment can be positive or negative\n",
    "train_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xHTMEyXW5KcQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# to restore words, load the word-to-index mapping\n",
    "word_to_index = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vv25lUc_5ckG"
   },
   "outputs": [],
   "source": [
    "# invert to get index-to-word mapping\n",
    "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CYX6F3AX5hpV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNKNOWN is an amazing actor and now the same being director UNKNOWN father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the UNKNOWN of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can convert a sequence to text by\n",
    "# - replacing each index by the respective word\n",
    "# - joining words together via spaces\n",
    "# note that we remove the beginning of sequence character and we have to subtract 3 from all indices\n",
    "# this is because, as mentioned above, the smallest indices are reserved for special characters\n",
    "# but for some reason this is not reflected in the mapping...\n",
    "\" \".join([index_to_word.get(index - 3, \"UNKNOWN\") for index in train_sequences[0][1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9axnbnwR6q6W"
   },
   "outputs": [],
   "source": [
    "# we cannot create a dataset :( this is because sequences are different length\n",
    "# but tensors have to be \"rectangular\"\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_sequences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2lt9mE-9XO7"
   },
   "outputs": [],
   "source": [
    "# solution is padding all sequences to the maximum length.\n",
    "# first find the maximum length\n",
    "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
    "max_len = max(sequence_lengths)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "677ZXcRu9nUe"
   },
   "outputs": [],
   "source": [
    "# overview over sequence lengths in the data\n",
    "# could also look at mean, median, standard deviation...\n",
    "plt.hist(sequence_lengths, bins=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYr10G5M9rWX"
   },
   "outputs": [],
   "source": [
    "# luckily there is a convenient function for padding\n",
    "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXEICggj-OL-"
   },
   "outputs": [],
   "source": [
    "# now we can create a dataset!\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPTPy5Ff-Q_C"
   },
   "outputs": [],
   "source": [
    "# all sequences are... very long\n",
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ug0OSIGjf6ji"
   },
   "outputs": [],
   "source": [
    "# it would be better to do something like this\n",
    "# all sequences above maxlen will be truncated to that length\n",
    "# note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!\n",
    "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
    "\n",
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZW7YdDv_fRJ"
   },
   "outputs": [],
   "source": [
    "# for fun, you can look at the word-index mappings.\n",
    "# in this case, the mapping was done according to word frequency.\n",
    "# you can pass reverse=True to sorted() to look at the least common words.\n",
    "sorted(index_to_word.items())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4fwUhqBACri"
   },
   "outputs": [],
   "source": [
    "# here is a high-level sketch for training RNNs\n",
    "\n",
    "\n",
    "# training loop -- same thing as before!!\n",
    "# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n",
    "# but all the related changes are hidden away at lower levels\n",
    "def train_loop():\n",
    "    for sequence_batch, label_batch in train_data:\n",
    "        train_step(sequence_batch, label_batch)\n",
    "\n",
    "\n",
    "# a single training step -- again, seems familiar?\n",
    "def train_step(sequences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = rnn_loop(sequences)\n",
    "        loss = loss_fn(labels, logits)\n",
    "\n",
    "    gradient = ...\n",
    "    apply_gradients(...)\n",
    "\n",
    "\n",
    "# here's where things start to change\n",
    "# we loop over the input time axis, and at each time step compute the new\n",
    "# hidden state based on the previous one as well as the current input\n",
    "# the state computation is hidden away in the rnn_step function and could be\n",
    "# arbitrarily complex.\n",
    "# in the general RNN, an output is computed at each time step, and the whole\n",
    "# sequence is returned. but in this case, since we only have one label for the\n",
    "# entire sequence, we only use the final state to compute one output and return it.\n",
    "# before the loop, the state need to be initialized somehow.\n",
    "def rnn_loop(sequences):\n",
    "    old_state = ...\n",
    "\n",
    "    for step in range(max_len):\n",
    "        x_t = sequences[:, step]\n",
    "        x_t = tf.one_hot(x_t, depth=num_words)\n",
    "        new_state = rnn_step(old_state, x_t)\n",
    "\n",
    "        old_state = new_state\n",
    "\n",
    "    o_t = output_layer(new_state)\n",
    "\n",
    "    return o_t\n",
    "\n",
    "\n",
    "# see formulas in the book ;)\n",
    "def rnn_step(state, x_t):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNRrNJS3Dbr71CnFVK3jax",
   "collapsed_sections": [],
   "name": "rnns_part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
