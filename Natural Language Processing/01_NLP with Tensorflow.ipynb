{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+aZYbzerJOgwF2LYN1cqq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## This File Explains: learning and practicing NLP with TensorFlow."],"metadata":{"id":"FmXhPkr-OhNL"}},{"cell_type":"markdown","source":["we will see how we can gain insights into text data and hands-on on how to use those insights to train NLP models and perform some human mimicking tasks."],"metadata":{"id":"JOp7a_IZOzGp"}},{"cell_type":"markdown","source":["**Tokenization:**\n","\n","Representing the words in a way that a computer can process them, with a view to later training a Neural network that can understand their meaning. This process is called tokenization.\n","\n","`Let’s look at how we can tokenize the sentences using TensorFlow tools.`"],"metadata":{"id":"CF6xYYMUO4qW"}},{"cell_type":"code","source":["# Importing required libraries\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","\n","# List of sample sentences that we want to tokenize\n","sentences = ['I love my dog.',\n","             'I love my cat?',\n","             ]\n","\n","# intializing a tokenizer that can index\n","# num_words is the maximum number words that can be kept \n","# tokenizer will automatically help in choosing most frequent words\n","tokenizer = Tokenizer(num_words = 100)\n","\n","# fitting the sentences to using created tokenizer object\n","tokenizer.fit_on_texts(sentences)\n","\n","# the full list of words is available as the tokenizer's word index\n","word_index = tokenizer.word_index\n","\n","# the result will be a dictionary, key being the words and the values being the token for that word\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKlb2zUcOmkO","executionInfo":{"status":"ok","timestamp":1681897160904,"user_tz":-330,"elapsed":406,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"1360a70a-3809-42c5-dc30-32116fced0ff"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"]}]},{"cell_type":"markdown","source":["The tokenizer is also smart enough to catch some exceptions. In the next example, we have added a word dog! but the tokenizer is smart enough to not create a new token for “dog!” again.\n","\n"],"metadata":{"id":"LYHYBNooP0rO"}},{"cell_type":"code","source":["sentences = ['I love my dog',\n","             'I love my cat',\n","             'you love my DOG!'\n","             ]\n","\n","tokenizer = Tokenizer(num_words = 100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","# Exoectec resulting dictionary without a new token for \"dog!\" \n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHK3z9RmPgez","executionInfo":{"status":"ok","timestamp":1681897271188,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"e2ccafab-f389-4399-9482-104397644c4f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}]},{"cell_type":"markdown","source":["**Sequencing:**\n","\n","Now that our words are represented like this, next, we need to represent our sentences by a sequence of numbers in the correct order. Then we will have data ready for processing by a neural network to understand or maybe even generate new text. Let’s look at how we can manage this sequencing using TensorFlow tools."],"metadata":{"id":"nb6X4HssQMNt"}},{"cell_type":"code","source":["sentences = ['I love my dog',\n","             'I love my cat',\n","             'you love my dog!',\n","             'Do you think my dog is amazing?',\n","             ]\n","\n","tokenizer = Tokenizer(num_words = 100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","# this creates sequence of tokens representing each sentence\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print(word_index)\n","print()\n","print(sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3nZwrSaP-mt","executionInfo":{"status":"ok","timestamp":1681897465463,"user_tz":-330,"elapsed":407,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"45b9368e-a7c2-43bb-d0e4-02177bdb1358"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n","\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"]}]},{"cell_type":"markdown","source":["Now we have basic tokenization done. But there is a catch. This is all very well for getting data ready for training a neural network, but what happens when that Neural Network has to classify texts, but there are words that it has never seen before? So this can confuse the Neural Network. Let’s look at how to handle that next.\n","\n","Let’s try sequencing a sentence, which has words that tokenizer has not seen yet."],"metadata":{"id":"3Szc3UXdQ9fH"}},{"cell_type":"code","source":["test_data = ['i really love my dog',\n","             'my dog loves my manatee',\n","             ]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print(test_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Dl8UOIjQePY","executionInfo":{"status":"ok","timestamp":1681897599072,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"81bbe785-a7bc-42f3-804a-2207327a1b12"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4, 2, 1, 3], [1, 3, 1]]\n"]}]},{"cell_type":"markdown","source":["**Unseen words:**\n","\n","`i really love my dog’ = [4, 2, 1, 3] i.e. a 5-word sentence ends up as a 4 numbered sequence, why?\n","Because the word “really” was not in the word index. The corpus used to build the word index doesn’t contain that word.\n","`\n","\n","`Similarly ‘my dog loves my manatee’ = [1, 3, 1] i.e. a 5-word sentence ends up as a 3 numbered sequence or it is equivalent to “my dog my” as “loves” and “manatee” are not in word index.`"],"metadata":{"id":"pwrw-OOARVjE"}},{"cell_type":"markdown","source":["So we can imagine that we need a huge word index to handle sentences that are not in the training set. But in order not to lose the length of the sequence, there is also a little trick that we can use. Let’s take a look at that."],"metadata":{"id":"gZpOxYO5R4Ex"}},{"cell_type":"markdown","source":["**OOV(out of vocabulary):**\n","\n","By using the OOV(out of vocabulary) token property, and setting it as something that you would not expect to see in the corpus, like “<OOV>”, this word is never used anywhere, so we can use a word that we can assume never appears in a text. Then the tokenizer will create a token for that and replaces words that it doesn’t recognize with the out of vocabulary token instead. It’s simple but effective. Let’s look at an example.\n"],"metadata":{"id":"i01yHiPISIHA"}},{"cell_type":"code","source":["sentences = ['I love my dog',\n","             'I love my cat',\n","             'you love my dog!',\n","             'Do you think my dog is amazing?',\n","             ]\n","\n","# adding a \"out of vocabulary\" word to the tokenizer\n","tokenizer = Tokenizer(num_words = 100,oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","test_data = ['i really love my dog',\n","             'my dog loves my manatee',\n","             ]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","\n","print(word_index)\n","print(test_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"im9pwc_URGCh","executionInfo":{"status":"ok","timestamp":1681897826714,"user_tz":-330,"elapsed":7,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"7cc63716-81e2-43aa-eaed-92eaffafe363"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"]}]},{"cell_type":"markdown","source":["Now we can notice that the length of the sentences has been retained and the unseen words in the sentence are replaced by the “<OOV>” token. So the resultant sentences are like :\n","\n","`‘i really love my dog’ = [5, 1, 3, 2, 4] = ‘i <OOV> love my dog’`\n","\n","`‘my dog loves my manatee’ = [2, 4, 1, 2, 1] =‘my dog <OOV> my <OOV>’`"],"metadata":{"id":"2k94gRJeShVC"}},{"cell_type":"markdown","source":["We still lost some meaning, but a lot less and the sentences are of at least the correct lengths. And while it helps to maintain the sequence length to be the same length as the sentence, we might wonder, when it comes to needing to train a Neural Network, how can it handle sentences of different lengths?\n","\n","With images, they are all usually the same size. So how would we solve that problem?"],"metadata":{"id":"qI6UPXjnTBwZ"}},{"cell_type":"markdown","source":["**Padding the sequences:**\n","\n","A simple solution is padding. For this, we will use pad_sequences imported for the sequence module of **`tensorflow.keras.preprocessing`.** As the name suggests, we can use it to pad our sequence. So we just need to pass sequences to pad_sequence function and the rest is done for us."],"metadata":{"id":"5QbceZVtTJi0"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = ['I love my dog',\n","             'I love my cat',\n","             'you love my dog!',\n","             'Do you think my dog is amazing?',\n","             ]\n","\n","tokenizer = Tokenizer(num_words = 100,oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","# padding sequences \n","padded = pad_sequences(sequences)\n","\n","print(word_index)\n","print()\n","print(sequences)\n","print()\n","print(padded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KMkMxHXSVui","executionInfo":{"status":"ok","timestamp":1681898164634,"user_tz":-330,"elapsed":483,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"4f3812c2-11c1-4989-fea2-30f8693fb05c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","\n","[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","\n","[[ 0  0  0  5  3  2  4]\n"," [ 0  0  0  5  3  2  7]\n"," [ 0  0  0  6  3  2  4]\n"," [ 8  6  9  2  4 10 11]]\n"]}]},{"cell_type":"markdown","source":["So our first example [5, 3, 2, 4] is preceded by 3 zeros in the padded sequence. But why 3 zeros? Well, it’s because our longest sentence has 7 words in it, so we pass this corpus sequence to pad sequence, it measures that and ensures that all of the sentences have equally-sized sequences by padding them with zero’s at the front. Note that OOV is 0, it is not 1."],"metadata":{"id":"GyrofkZQUYGR"}},{"cell_type":"markdown","source":["Now we might think that we don’t want zero’s in the front, but instead after the sentence. Well, that’s easy. We can just the padding parameter to “post” i.e **padding = “post”**."],"metadata":{"id":"onGT1bg6UgGL"}},{"cell_type":"markdown","source":["Or if we don’t want the length of the padded sentences to be the same as the longest sentence, we can then specify the desired length by specifying the “maxlen” parameter to the required length. But wait? we might think what happens if the sentences are longer than the “maxlen” parameter?"],"metadata":{"id":"LU4TRgRaUqQD"}},{"cell_type":"markdown","source":["Well, then we can specify hot to truncate the sentence whether by chopping off the words at the end, with a post truncation or from the beginning with a pre-truncation. Please refer to pad_sequences documentation for other options."],"metadata":{"id":"tlwrXpHrUyAJ"}},{"cell_type":"markdown","source":["The function pad_Sequences might then look like :\n","\n","`padded = pad_sequences(sequences,maxlen = 5, padding=’post’, truncating = ‘post’)`"],"metadata":{"id":"VV2_A313VHqf"}},{"cell_type":"markdown","source":["Till now we have seen how to tokenize text into numeric values, and use tools in TensorFlow to regularize and pad that text. Now that we’ve gotten pre-processing out of the way, we can next look at how to build a classifier to recognize sentiment in text."],"metadata":{"id":"IzW_E8BYVg1w"}},{"cell_type":"markdown","source":["We’ll start by using a [dataset of News headlines](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection), where the headlines have been categorized as sarcastic or not. We’ll train a classifier on this and it can then tell us that if a new piece of text looks like it might be sarcastic or not."],"metadata":{"id":"eW2lKO6gVinV"}},{"cell_type":"markdown","source":["**This dataset has 3 fields:**\n","\n","`is_sarcastic field: “1” if sarcastic and 0 otherwise.`\n","\n","`headline: the headline if the news article`\n","\n","`article_link: link to the original news article.`"],"metadata":{"id":"YIMZltTCVzKz"}},{"cell_type":"markdown","source":["The data is stored in JSON format and we will convert it to python DataFrame format for training."],"metadata":{"id":"ymjZSNu9WNn-"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiHo-aZ6-uCp","executionInfo":{"status":"ok","timestamp":1681898843731,"user_tz":-330,"elapsed":20748,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"98d9a886-a0f4-4765-d369-130378bc7b42"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["path = \"/content/gdrive/MyDrive/Deep Learning/Jupyter Notebook/Natural Language Processing/Sarcasm_Headlines_Dataset.json\""],"metadata":{"id":"kId8BoyZ-5CJ","executionInfo":{"status":"ok","timestamp":1681898930225,"user_tz":-330,"elapsed":439,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# import os\n","# path = os.getcwd()\n","\n","data = pd.read_json(path, lines=True)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"GipX9UfuZqDl","executionInfo":{"status":"ok","timestamp":1681898951458,"user_tz":-330,"elapsed":930,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"57fc0f97-ee4f-43df-c099-5c75138ccfe5"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                        article_link  \\\n","0  https://www.huffingtonpost.com/entry/versace-b...   \n","1  https://www.huffingtonpost.com/entry/roseanne-...   \n","2  https://local.theonion.com/mom-starting-to-fea...   \n","3  https://politics.theonion.com/boehner-just-wan...   \n","4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n","\n","                                            headline  is_sarcastic  \n","0  former versace store clerk sues over secret 'b...             0  \n","1  the 'roseanne' revival catches up to our thorn...             0  \n","2  mom starting to fear son's web series closest ...             1  \n","3  boehner just wants wife to listen, not come up...             1  \n","4  j.k. rowling wishes snape happy birthday in th...             0  "],"text/html":["\n","  <div id=\"df-94095a8c-f264-4d13-9189-ad8f0c9597aa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>article_link</th>\n","      <th>headline</th>\n","      <th>is_sarcastic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n","      <td>former versace store clerk sues over secret 'b...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n","      <td>the 'roseanne' revival catches up to our thorn...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n","      <td>mom starting to fear son's web series closest ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>https://politics.theonion.com/boehner-just-wan...</td>\n","      <td>boehner just wants wife to listen, not come up...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n","      <td>j.k. rowling wishes snape happy birthday in th...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94095a8c-f264-4d13-9189-ad8f0c9597aa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-94095a8c-f264-4d13-9189-ad8f0c9597aa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-94095a8c-f264-4d13-9189-ad8f0c9597aa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# training_size = 20000\n","X, y = data['headline'], data['is_sarcastic']\n","from sklearn.model_selection import train_test_split\n","training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(X, y, test_size=0.25)\n"],"metadata":{"id":"GMlI9co3agyI","executionInfo":{"status":"ok","timestamp":1681899004834,"user_tz":-330,"elapsed":1458,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["training_sentences.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISmtoGrTbvCE","executionInfo":{"status":"ok","timestamp":1681899008189,"user_tz":-330,"elapsed":406,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"3a7bb37d-4e61-4be9-b24a-129fb90323c4"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20031,)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["training_labels.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hVcRqEUNbzei","executionInfo":{"status":"ok","timestamp":1681899020858,"user_tz":-330,"elapsed":400,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"9419dabb-5ab5-4ffb-c9b2-89fb19b26cb4"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20031"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["vocab_size = 10000\n","max_length = 100\n","trunc_type='post'\n","padding_type='post'\n","oov_token = \"<OOV>\"\n","training_size = training_labels.shape[0]"],"metadata":{"id":"hjxp4RtwTvV6","executionInfo":{"status":"ok","timestamp":1681899117512,"user_tz":-330,"elapsed":4,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["training_sentences.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHt1YxiIgf4u","executionInfo":{"status":"ok","timestamp":1681899075820,"user_tz":-330,"elapsed":6,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"b1519673-2628-4ac1-93e1-b688c7c48d45"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20031,)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n","# fitting tokenizer only to training set\n","tokenizer.fit_on_texts(training_sentences)\n","\n","word_index = tokenizer.word_index\n","\n","# creating training sequences and padding them\n","training_sequences = tokenizer.texts_to_sequences(training_sentences)\n","training_padded = pad_sequences(training_sequences,maxlen = max_length,\n","                                padding = padding_type,\n","                                truncating=trunc_type,\n","                                )\n","\n","# creating  testing sequences and padding them using same tokenizer\n","testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n","testing_padded = pad_sequences(testing_sequences,maxlen = max_length,\n","                                padding = padding_type,\n","                                truncating=trunc_type,\n",")\n","\n","                              \n","print(training_padded.shape, testing_padded.shape)\n","import numpy as np\n","# converting all variables to numpy arrays, to be able to work with tf version 2\n","training_padded = np.array(training_padded)\n","training_labels = np.array(training_labels)\n","testing_padded = np.array(testing_padded)\n","testing_labels = np.array(testing_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3j2dYNsYAQt","executionInfo":{"status":"ok","timestamp":1681899213742,"user_tz":-330,"elapsed":1946,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"9ad7425a-29de-44f0-ebd0-81b1ce5329dc"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(20031, 100) (6678, 100)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"VbU-CzTyATe2"}},{"cell_type":"code","source":["training_padded.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CqbFGR1Lf0F6","executionInfo":{"status":"ok","timestamp":1681899244020,"user_tz":-330,"elapsed":559,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"23edf4cd-8989-43e7-a5e1-1c7680a4661b"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20031, 100)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["training_labels.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOQumwspgR4c","executionInfo":{"status":"ok","timestamp":1681899245656,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"f88a9d75-53a8-462d-d923-eaacad929c7f"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20031,)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["word_index[\"<OOV>\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-xvQ_1lAlc3","executionInfo":{"status":"ok","timestamp":1681899302292,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"e51ef487-be72-4aea-ee62-8e8824d74b28"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["training_padded[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_UxsXpYAfLo","executionInfo":{"status":"ok","timestamp":1681899263832,"user_tz":-330,"elapsed":6,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"3a93f393-1842-4359-d100-962751393597"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2620, 2850,   32,    1,   17,   99,  213,    1, 1366,  702,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0], dtype=int32)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["**Word Embeddings:**\n","\n","But you might be wondering that we’ve turned our sentences to numbers, with numbers being tokens representing the words. But how do we get meaning from that? How do we determine if something is sarcastic just from the numbers?\n","\n","Well, here’s where the context of embeddings comes in.\n","\n","Let’s consider the most basic of sentiments, good and bad. We can often see these as being opposites, so we can plot them as having opposite directions as shown in the below image.\n","\n","So then what happens with a word like “meh”? it’s not particularly good, and it is not particularly bad. Probably a little more bad than good. So we can plot it somewhere near the bad line. Or the phrase “not bad” which is usually meant to plot something as having a little bit of goodness, but not necessarily very good. So this plot can be inclined towards the good line.\n","\n","[link text](https://drive.google.com/file/d/13RD8Lj9gqDFDH4kjhXcPqt99WeDZ7jdq/view?usp=share_link)"],"metadata":{"id":"o4S4a_gVd_Bn"}},{"cell_type":"markdown","source":["Now imagine plotting this on the X and Y axis, then we can start to determine the good or bad sentiment as the coordinates in the X and Y as shown in the image(image not to scale). Similarly, we can represent “meh” and good as points in the XY plane."],"metadata":{"id":"5ntatJbQfBHM"}},{"cell_type":"markdown","source":["So by looking at the direction of the vector, we can start to determine the meaning of the word. So what if we can extend that into multiple dimensions instead of just two? What if words that are labeled with sentiments, like sarcastic and not sarcastic, are plotted in a multi-dimensional space. And then as we train, we try to learn what the direction in these multi-dimensional spaces should look like. Words that appear only in the sarcastic sentences will have a strong component in the sarcastic direction and vice versa."],"metadata":{"id":"R8yRsgWMfCU8"}},{"cell_type":"markdown","source":["As we load more and more sentences into the network for training, these directions can change. And when we have a fully trained network and give it a set of words, it could look up the vectors for these words, sum them up, and thus give us an idea for the sentiment. This concept is known as embedding."],"metadata":{"id":"dVhuCn_afG4w"}},{"cell_type":"markdown","source":["Now let’s take a look at how we can do this using the TensorFlow embedding layer."],"metadata":{"id":"-2qeLZNqfKlL"}},{"cell_type":"code","source":["embedding_dim = 16\n","\n","# creating a model for sentiment analysis\n","model  = tf.keras.Sequential([\n","                # addinging an Embedding layer for Neural Network to learn the vectors\n","                tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n","                # Global Average pooling is similar to adding up vectors in this case\n","                tf.keras.layers.GlobalAveragePooling1D(),\n","                tf.keras.layers.Dense(24, activation = 'relu'),\n","                tf.keras.layers.Dense(1, activation = 'sigmoid')\n","])\n","\n","model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","\n","\n","num_epochs = 10\n","\n","history = model.fit(training_padded,training_labels, epochs = num_epochs,\n","                    validation_data = (testing_padded,testing_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rhvk24dOdyz2","executionInfo":{"status":"ok","timestamp":1681900212010,"user_tz":-330,"elapsed":37347,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"9bf9adbe-60fc-4f19-f22e-3dbb0f6181ea"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","626/626 [==============================] - 4s 5ms/step - loss: 0.6680 - accuracy: 0.5862 - val_loss: 0.6042 - val_accuracy: 0.7974\n","Epoch 2/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.4809 - accuracy: 0.8158 - val_loss: 0.4013 - val_accuracy: 0.8362\n","Epoch 3/10\n","626/626 [==============================] - 7s 11ms/step - loss: 0.3464 - accuracy: 0.8649 - val_loss: 0.3494 - val_accuracy: 0.8507\n","Epoch 4/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.2890 - accuracy: 0.8872 - val_loss: 0.3433 - val_accuracy: 0.8480\n","Epoch 5/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.2500 - accuracy: 0.9053 - val_loss: 0.3246 - val_accuracy: 0.8585\n","Epoch 6/10\n","626/626 [==============================] - 4s 6ms/step - loss: 0.2214 - accuracy: 0.9169 - val_loss: 0.3394 - val_accuracy: 0.8531\n","Epoch 7/10\n","626/626 [==============================] - 4s 6ms/step - loss: 0.1995 - accuracy: 0.9252 - val_loss: 0.3307 - val_accuracy: 0.8597\n","Epoch 8/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.1791 - accuracy: 0.9353 - val_loss: 0.3397 - val_accuracy: 0.8603\n","Epoch 9/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.1634 - accuracy: 0.9398 - val_loss: 0.3529 - val_accuracy: 0.8559\n","Epoch 10/10\n","626/626 [==============================] - 3s 5ms/step - loss: 0.1503 - accuracy: 0.9473 - val_loss: 0.3690 - val_accuracy: 0.8518\n"]}]},{"cell_type":"markdown","source":["We can notice we achieved some good accuracy with training data, but as we can see the val_accuracy is decreasing, which is some classic overfitting. So we can either add less learning rate to our model or train for less number of epochs."],"metadata":{"id":"LDgwyEoOjnas"}},{"cell_type":"markdown","source":["**Establishing Sentiment:**\n","\n","Now let us see how we can use this model to establish sentiment for unseen sentences."],"metadata":{"id":"nxkWV-sZjsKE"}},{"cell_type":"code","source":["# forming new sentences for testing, feel free to experiment\n","# sentence 1 is bit sarcastic, whereas sentence two is a general statment.\n","new_sentence = [\n","                \"granny starting to fear spider in the garden might be real\",\n","                \"game of thrones season finale showing this sunday night\"]\n","\n","# Converting the sentences to sequences using tokenizer\n","new_sequences = tokenizer.texts_to_sequences(new_sentence)\n","# padding the new sequences to make them have same dimensions\n","new_padded = pad_sequences(new_sequences, maxlen = max_length,\n","                           padding = padding_type,\n","                           truncating = trunc_type)\n","\n","new_padded = np.array(new_padded )\n","\n","print(model.predict(new_padded))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghBzgwuxfaCY","executionInfo":{"status":"ok","timestamp":1681900294872,"user_tz":-330,"elapsed":395,"user":{"displayName":"Raj Yadav","userId":"17883966751087890537"}},"outputId":"52daa4bc-8a8e-4730-f4da-d4174f79cb2b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 101ms/step\n","[[0.28417525]\n"," [0.2538942 ]]\n"]}]},{"cell_type":"markdown","source":["*0.7923 indicates that the first sentence has 72% cahnce of being a sarcastic sentence, so it is classified as a sarcastic one, whereas 0.0628 indicates that the second sentence is very close to a non-sarcastic one.*"],"metadata":{"id":"6XqkelsIkEDZ"}},{"cell_type":"code","source":[],"metadata":{"id":"H7VwLwQEj1VF"},"execution_count":null,"outputs":[]}]}