# -*- coding: utf-8 -*-
"""Linear Model for MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_HIsgSRLIBQ0Y65IpfDmc3cO7yhEu_F

## **Imports**
"""

from google.colab import drive
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
import numpy as np
from numpy.random.mtrand import randint
from numpy.core.multiarray import dtype

"""**Loading data and sanity checking. We make use of the “built-in” MNIST data in Tensorflow**"""

class MNISTDataset:
    """'Bare minimum' class to wrap MNIST numpy arrays into a dataset."""
    def __init__(self, train_imgs, train_lbs, test_imgs, test_lbls, batch_size,
                 to01=True, shuffle=True, seed=None):
        """
        Use seed optionally to always get the same shuffling (-> reproducible
        results).
        """
        self.batch_size = batch_size
        self.train_data = train_imgs
        self.train_labels = train_lbs.astype(np.int32)
        self.test_data = test_imgs
        self.test_labels = test_lbls.astype(np.int32)

        if to01:
            # int in [0, 255] -> float in [0, 1]
            self.train_data = self.train_data.astype(np.float32) / 255
            self.test_data = self.test_data.astype(np.float32) / 255

        self.size = self.train_data.shape[0]

        if seed:
            np.random.seed(seed)
        if shuffle:
            self.shuffle_train()
        self.shuffle = shuffle
        self.current_pos = 0

    def next_batch(self):
        """Either gets the next batch, or optionally shuffles and starts a
        new epoch."""
        end_pos = self.current_pos + self.batch_size
        if end_pos < self.size:
            batch = (self.train_data[self.current_pos:end_pos],
                     self.train_labels[self.current_pos:end_pos])
            self.current_pos += self.batch_size
        else:
            # we return what's left (-> possibly smaller batch!) and prepare
            # the start of a new epoch
            batch = (self.train_data[self.current_pos:self.size],
                     self.train_labels[self.current_pos:self.size])
            if self.shuffle:
                self.shuffle_train()
            self.current_pos = 0
            print("Starting new epoch...")
        return batch

    def shuffle_train(self):
        shuffled_inds = np.arange(self.train_data.shape[0])
        np.random.shuffle(shuffled_inds)
        self.train_data = self.train_data[shuffled_inds]
        self.train_labels = self.train_labels[shuffled_inds]

mnist = tf.keras.datasets.mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

"""**We plot the first training image just so we know what we are dealing with, also plotting random index from MNIST**"""

plt.imshow(train_images[randint(0,10)], cmap="Greys_r")

"""**Next, we create a dataset via our simple wrapper, using a batch size of 128. Be aware that the data is originally represented as uint8 in the range [0, 255] but MNISTDataset converts it to float32 in [0,1] by default. Also, labels are converted from uint8 to int32.**"""

data = MNISTDataset(train_images.reshape([-1, 784]), train_labels,
                    test_images.reshape([-1, 784]), test_labels,
                    batch_size=128)

"""## **Setting up for training**
We decide on the number of training steps and the learning rate, and set up our weights to be trained with random initial values (and zero biases).
"""

train_steps = 1000
learning_rate = 0.1

W = tf.Variable(np.zeros([784, 10]).astype(np.float32))
b = tf.Variable(np.zeros(10, dtype=np.float32))

"""## **Training**
The main training loop, using cross-entropy as a loss function. We regularly print the current loss and accuracy to check progress.

Note that we compute the “logits”, which is the common name for pre-softmax values. They can be interpreted as log unnormalized probabilities and represent a “score” for each class.

In computing the accuracy, notice that we have to fiddle around with dtypes quite a bit — this is unfortunately common in Tensorflow.
"""

for step in range(train_steps):
    img_batch, lbl_batch = data.next_batch()
    with tf.GradientTape() as tape:
        logits = tf.matmul(img_batch, W) + b
        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=lbl_batch))

    grads = tape.gradient(xent, [W, b])
    W.assign_sub(learning_rate * grads[0])
    b.assign_sub(learning_rate * grads[1])

    if not step % 100:
        preds = tf.argmax(logits, axis=1, output_type=tf.int32)
        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),
                             tf.float32))
        print("Loss: {} Accuracy: {}".format(xent, acc))

"""## **Predicting/testing**
i have use the trained model to predict labels on the test set and check the model’s accuracy
"""

test_preds = tf.argmax(tf.matmul(data.test_data, W) + b,
                      axis=1,
                      output_type=tf.int32)

acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels), tf.float32))

print(acc)